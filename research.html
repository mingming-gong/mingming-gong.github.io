

<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link rel="stylesheet" type="text/css" href="style.css" />
    <title>Mingming Gong</title>
    <base href="https://mingming-gong.github.io/index.html">
</head>

<body>
<h1 style="padding-left: 0.5em">Mingming Gong</h1><hr>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
    <div class="menu-item"><a href="index.html" class="current">Home</a></div>
    <div class="menu-item"><a href="news.html">News</a></div>
    <div class="menu-item"><a href="students.html">Students</a></div>
    <div class="menu-item"><a href="research.html">Research</a></div>
    <div class="menu-item"><a href="publications.html">Publications</a></div>
    <div class="menu-item"><a href="teaching.html">Teaching</a></div>
    <div class="menu-item"><a href="talks.html">Talks</a></div>
    <div class="menu-item"><a href="service.html">Service</a></div>
    <div class="menu-item"><a href="awards.html">Awards & Honors</a></div>
</td>
<td id="layout-content">

<h1 style="margin-top: 0em">Research (Selected Topics)</h1>

<div>
    <h2><hr>Causal Discovery & Inference</h2>
            <img src="cd.jpg" align="right" border="0" width=350/>
    Causal discovery and inference involve the challenge of learning causal structures from observational data and making inferences about causal effects based on these structures. This task is foundational across diverse scientific and engineering disciplines. Our current focus revolves around developing computational methods capable of uncovering causal relationships within intricate real-world datasets, all while ensuring theoretical robustness and reliability.
    <br>
    <br> 
        <ul>
        <li><p>
        Causal discovery from imperfectly sampled time series data [<A HREF="papers/ICML_SUBSAMPLE.pdf">ICML'15</A>][<A HREF="https://arxiv.org/pdf/1411.3972.pdf">ICML'15</A>][<A HREF="papers/UAI_CDTA.pdf">UAI'17</A>][<A HREF="https://papers.nips.cc/paper/8912-likelihood-free-overcomplete-ica-and-applications-in-causal-discovery.pdf">NeurIPS'19</A>]
        </p></li>
        <li><p>
        Causal discovery from nonstationary data [<A HREF="http://proceedings.mlr.press/v97/huang19g/huang19g.pdf">ICML'19</A>]
        </p></li>
        <li><p>
        Causal discovery from data with measurement error [<A HREF="http://auai.org/uai2018/proceedings/papers/372.pdf">UAI'18</A>]
        </p></li>
        <li><p>
        Causal discovery from data with missing values [<A HREF="https://www.aaai.org/Papers/AAAI/2020GB/AAAI-HuangB.6175.pdf">AAAI'20</A>][<A HREF="https://arxiv.org/pdf/2205.13869.pdf">NeurIPS'22</A>]
        </p></li>
        <li><p>
        Federated causal discovery [<A HREF="https://arxiv.org/abs/2112.03555">TMLR'23</A>]
        </p></li>
        <li><p>
        Scalable search [<A HREF="https://arxiv.org/pdf/2208.14571">NeurIPS'22</A>][<A HREF="">CLeaR'24</A>]
        </p></li>
        <li><p>
        Causal effect estimation [<A HREF="https://openreview.net/pdf?id=S46Knicu56">ICLR'24</A>]
        </p></li>
        </ul>
        <br>
</div>

<div>
    <h2><hr>Causal Representation Learning</h2>
            <img src="DA.jpg" align="right" border="0" width=350/>
Powered by deep neural networks, representation learning extracts high-level representations from raw data and enables accurate predictions in many applications. However, existing representation learning techniques can not outperform humans in transferring/generalizing to new environments and tasks. In addition, the blackbox representations face challenges in the social aspects of AI, such as fairness, reliablity, and transparency. Our research goal is to learn causal representations, often via causal generative models, that enable efficient transfer learning, out-of-distribution generalization, and socially responsible learning.
    <br> 
    
    <ul>
    <li><p>
    Domain adaptation/generalization [<A HREF="papers/AAAI_MULTI.pdf">AAAI'15</A>][<A HREF="papers/ICML_CTC.pdf">ICML'16</A>][<A HREF="https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/16595/16558">AAAI'18</A>][<A HREF="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ya_Li_Deep_Domain_Generalization_ECCV_2018_paper.pdf">ECCV'18</A>]
    [<A HREF="http://proceedings.mlr.press/v89/stojanov19b/stojanov19b.pdf">AISTATS'19</A>][<A HREF="https://proceedings.icml.cc/static/paper_files/icml/2020/1262-Paper.pdf">ICML'20</A>][<A HREF="https://proceedings.icml.cc/static/paper_files/icml/2020/1942-Paper.pdf">ICML'20</A>] 
    [<A HREF="https://papers.nips.cc/paper/2020/file/b98249b38337c5088bbc660d8f872d6a-Paper.pdf">NeurIPS'20</A>][<A HREF="https://arxiv.org/abs/2002.03278">NeurIPS'20</A>][<A HREF="https://openreview.net/forum?id=zdmF437BCB">NeurIPS'21</A>]
     </p></li>
    <li><p> 
    Fair representation learning [<A HREF="https://openreview.net/forum?id=s-pcpETLpY">CLeaR'22</A>][<A HREF="https://arxiv.org/pdf/2205.13972">NeurIPS'22</A>][<A HREF="https://mingming-gong.github.io/publications.html">ICLR'24</A>]       
    </p></li>
    <li><p> 
    Robust machine learning [<A HREF="https://arxiv.org/abs/2109.02986">NeurIPS'21</A>][<A HREF="https://openreview.net/forum?id=cZAi1yWpiXQ">ICLR'22</A>] [<A HREF="https://arxiv.org/abs/2207.03162">ICLR'23</A>][<A HREF="https://openreview.net/forum?id=sDCMrYnXNGY">ICML'23</A>]    
    </p></li>   
    <li><p> 
    Generalization in reinforcement learning [<A HREF="https://openreview.net/forum?id=YRq0ZUnzKoZ">ICLR'22</A>]    
    </p></li>
    <li><p>
    Unsupervised image-to-image translation [<A HREF="https://arxiv.org/abs/2109.11736">ICCV'21</A>][<A HREF="https://openreview.net/pdf?id=U2g8OGONA_V">ICLR'23</A>]
    </p></li>
    <li><p>
    Non-Transferable Representation Learning [<A HREF="https://openreview.net/pdf?id=FYKVPOHCpE">ICLR'24</A>]
    </p></li>
    </ul>
    <br>

</div>
<div>

    <h2><hr>Generative Models</h2>
            <img src="facegeneration.jpg" align="right" border="0" width=250/>

        Generative models, including GPT, GANs, and diffusion models, wield significant neural network capabilities to faithfully replicate intricate distributions found in real data. Our research emphasis lies in crafting generative models that are not only data-efficient but also computationally efficient. Moreover, delving into the realm of causal generative models, our core interest centers on developing models that mimic the data generation process while affording controllable and nuanced generations. Lastly, our curiosity extends to exploring generative models tailored for diverse data types, spanning images, text, 3D human motion, and beyond.
    <br>
    <br>

    <ul>
    <li><p>
    Fundamental models [<A HREF="https://arxiv.org/abs/1907.02690">NeurIPS'19</A>][<A HREF="https://arxiv.org/abs/2306.12511">NeurIPS'23</A>]
    </p></li>
    <li><p>
    Causal generative models [<A HREF="https://openreview.net/forum?id=U2g8OGONA_V">ICLR'23</A>][<A HREF="https://openreview.net/pdf?id=ia9fKO1Vjq">ICLR'24</A>]
    </p></li>
    <li><p>
    Image generation [<A HREF="https://arxiv.org/abs/1809.05852">CVPR'19</A>][<A HREF="https://arxiv.org/abs/2109.11736">ICCV'21</A>][<A href="https://arxiv.org/abs/2203.12707">CVPR'22</A>][<A href="https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Alleviating_Semantics_Distortion_in_Unsupervised_Low-Level_Image-to-Image_Translation_via_Structure_CVPR_2022_paper.pdf">CVPR'22</A>][<A HREF="https://openreview.net/pdf?id=ia9fKO1Vjq">ICLR'24</A>]
    </p></li>
    <li><p>
    3D human motion generation [<A HREF="https://arxiv.org/pdf/2312.12227.pdf">AAAI'24</A>][<A HREF="https://arxiv.org/pdf/2401.03476.pdf">ICASSP'24</A>]
    </p></li>
    <li><p>
    3D medical image generation [<A HREF="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770375">IEEE JBHI</A>]
    </p></li>
    <li><p>
    Font generation [<A href="https://openaccess.thecvf.com/content/CVPR2022/papers/Tang_Few-Shot_Font_Generation_by_Learning_Fine-Grained_Local_Styles_CVPR_2022_paper.pdf" >CVPR'22</A>] 
    </p></li>
    </ul>
    <br>
</div>

<div>

    <h2><hr>3D Vision</h2>
    <img src="3D.png" align="right" border="0" width=320/>

    The realm of 3D vision is dedicated to modeling and comprehending the visual world, with a primary focus on deducing three-dimensional structures from two-dimensional images. Our research explores machine learning approaches for various facets of 3D vision, including depth estimation, novel view synthesis, feature matching, SLAM, and 3D model retrieval.    
    <br>
    <br>
    
    <ul>
    <li><p>
    Depth estimation [<A HREF="https://hal.archives-ouvertes.fr/hal-01741163/file/CVPR18_DepthEstimation.pdf">CVPR'18</A>][<A HREF="https://arxiv.org/pdf/1904.01870">CVPR'19</A>][<A HREF="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136620229.pdf">ECCV'22</A>]
    </p></li>
    <li><p>
    Novel view synthesis [<A HREF="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136750722.pdf">ECCV'22</A>][<A HREF="https://arxiv.org/abs/2304.10075">ICCV'23</A>]
    </p></li>
    <li><p>
    3D model retrieval [<A HREF="https://arxiv.org/abs/2010.12238">NeurIPS'20</A>]
    </p></li>
    <li><p>
    Feature points & matching [<A HREF="https://link.springer.com/article/10.1007/s11263-023-01837-3">IJCV'23</A>][<A HREF="https://link.springer.com/article/10.1007/s11263-021-01568-3">IJCV'22</A>][<A HREF="https://arxiv.org/abs/2303.17981">ICRA'23</A>]
    </p></li>
    <li><p>
    3D dataset [<A HREF="https://arxiv.org/abs/2009.09633">IJCV'21</A>]
    </p></li>
    </ul>
    <br>
</div>


</td>
</tr>
</table>
</body>
</html>
